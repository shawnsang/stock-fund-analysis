"""
LLMé›†æˆæ¨¡å—
å¤„ç†æç¤ºè¯ç”Ÿæˆå’ŒAPIè°ƒç”¨
"""

from typing import Iterator, Optional
import openai
from .config import config
from .logger import get_logger

logger = get_logger(__name__)


class LLMClient:
    """LLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        self.client = openai.OpenAI(
            api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL
        )
        self.model = config.OPENAI_MODEL
        
    def generate_analysis_prompt(self, stock_code: str, market: str, markdown_data: str) -> str:
        """
        ç”Ÿæˆèµ„é‡‘æµåˆ†ææç¤ºè¯
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            market: å¸‚åœºä»£ç 
            markdown_data: Markdownæ ¼å¼çš„æ•°æ®è¡¨æ ¼
            
        Returns:
            å®Œæ•´çš„åˆ†ææç¤ºè¯
        """
        prompt = f"""
## åˆ†ææ•°æ®
ä»¥ä¸‹æ˜¯è¯¥è‚¡ç¥¨{stock_code}({market.upper()})æœ€è¿‘30ä¸ªäº¤æ˜“æ—¥çš„èµ„é‡‘æµæ•°æ®ï¼ŒåŒ…å«å‡€é¢ï¼ˆäº¿å…ƒï¼‰å’Œå‡€å æ¯”ï¼ˆ%ï¼‰ï¼š

**é‡è¦è¯´æ˜ï¼šæ•°æ®æŒ‰æ—¥æœŸé™åºæ’åˆ—ï¼Œè¡¨æ ¼ç¬¬ä¸€è¡Œä¸ºæœ€æ–°äº¤æ˜“æ—¥æ•°æ®ï¼Œæœ€åä¸€è¡Œä¸ºæœ€æ—©äº¤æ˜“æ—¥æ•°æ®ã€‚**

{markdown_data}

## åˆ†æè¦æ±‚
åŸºäº30æ—¥èµ„é‡‘æµæ•°æ®ï¼Œç®€è¦å›ç­”ï¼š

**æŠ•èµ„å»ºè®®**: 
ğŸ”´ ä¹°å…¥/ğŸŸ¡ æŒæœ‰/ğŸ”µ è§‚æœ›/ğŸŸ  å‡ä»“/âš« å–å‡º
ã€æ“ä½œç­–ç•¥ã€‘ï¼šæ“ä½œæ–¹å‘ï¼Ÿé‡ç‚¹è§‚å¯ŸæŒ‡æ ‡ï¼Ÿä¹°å…¥æˆ–å–å‡ºä»·ä½ï¼Ÿ
- çŸ­æœŸç­–ç•¥ï¼ˆ1-3æ—¥ï¼‰ï¼š
- ä¸­æœŸç­–ç•¥ï¼ˆ1-2å‘¨ï¼‰ï¼š
**è¶‹åŠ¿**: ä¸»åŠ›èµ„é‡‘æµå‘ï¼ŸMAå‡çº¿æ–¹å‘ï¼Ÿ
- å»ºä»“/å¢ä»“/å‡ä»“/å‡ºè´§
**ç»“æ„**: æœºæ„vsæ•£æˆ·ä¸»å¯¼ï¼Ÿèµ„é‡‘ååŒæ€§ï¼Ÿ

ã€é£é™©ç­‰çº§ã€‘ï¼šğŸ”´é«˜é£é™©/ğŸŸ ä¸­é«˜é£é™©/ğŸŸ¡ä¸­ç­‰é£é™©/ğŸŸ¢ä¸­ä½é£é™©/ğŸ”µä½é£é™©

è¦æ±‚ï¼šæ•°æ®æ”¯æ’‘ï¼Œç»“è®ºç®€æ˜ï¼Œçªå‡ºé‡ç‚¹ã€‚"""


        return prompt
    
    def analyze_fund_flow(self, stock_code: str, market: str, markdown_data: str) -> Iterator[str]:
        """
        åˆ†æèµ„é‡‘æµæ•°æ®å¹¶è¿”å›æµå¼ç»“æœ
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            market: å¸‚åœºä»£ç 
            markdown_data: Markdownæ ¼å¼çš„æ•°æ®è¡¨æ ¼
            
        Yields:
            åˆ†æç»“æœçš„æ–‡æœ¬ç‰‡æ®µ
        """
        try:
            # ç”Ÿæˆæç¤ºè¯
            prompt = self.generate_analysis_prompt(stock_code, market, markdown_data)
            
            # è®°å½•å®Œæ•´æç¤ºè¯åˆ°æ—¥å¿—
            logger.info("=== LLMåˆ†ææç¤ºè¯ ===")
            logger.info(prompt)
            logger.info("=== æç¤ºè¯ç»“æŸ ===")
            
            # è°ƒç”¨LLM API
            logger.info(f"å¼€å§‹è°ƒç”¨LLMåˆ†æè‚¡ç¥¨ {stock_code} çš„èµ„é‡‘æµæ•°æ®")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system", 
                        "content": """ä¸“ä¸šè‚¡ç¥¨èµ„é‡‘æµåˆ†æå¸ˆã€‚åˆ†æè¦æ±‚ï¼š
1. **è¶‹åŠ¿**: èµ„é‡‘æµå‘å’ŒMAå‡çº¿æ–¹å‘
2. **ç»“æ„**: æœºæ„vsæ•£æˆ·ï¼ŒååŒvsèƒŒç¦»  
3. **ä¿¡å·**: ä¹°å–ä¿¡å·å’Œé£é™©ç‚¹
4. **å»ºè®®**: å…·ä½“æ“ä½œå’Œè§‚å¯Ÿé‡ç‚¹

è¾“å‡ºè¦æ±‚ï¼šç»“è®ºå…ˆè¡Œï¼Œæ•°æ®æ”¯æ’‘ï¼Œç®€æ˜æ‰¼è¦ã€‚"""
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                stream=True,
                temperature=0.5,
                max_tokens=1500
            )
            
            # æµå¼è¿”å›ç»“æœ
            for chunk in response:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    yield content
                    
        except Exception as e:
            error_msg = f"LLMåˆ†æå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            yield f"âŒ {error_msg}"
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•LLMè¿æ¥
        
        Returns:
            è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("æµ‹è¯•LLMè¿æ¥")
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10
            )
            
            logger.info("LLMè¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"LLMè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")
            return False


def create_llm_client() -> Optional[LLMClient]:
    """
    åˆ›å»ºLLMå®¢æˆ·ç«¯å®ä¾‹
    
    Returns:
        LLMå®¢æˆ·ç«¯å®ä¾‹ï¼Œå¦‚æœé…ç½®æ— æ•ˆåˆ™è¿”å›None
    """
    if not config.validate():
        missing_configs = config.get_missing_configs()
        logger.error(f"LLMé…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘: {', '.join(missing_configs)}")
        return None
    
    try:
        client = LLMClient()
        if client.test_connection():
            logger.info("LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            return client
        else:
            logger.error("LLMå®¢æˆ·ç«¯è¿æ¥å¤±è´¥")
            return None
            
    except Exception as e:
        logger.error(f"åˆ›å»ºLLMå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        return None